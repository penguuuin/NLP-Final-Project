{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from convokit import Corpus, Speaker, Utterance\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8767236356574092\n"
     ]
    }
   ],
   "source": [
    "# Prepare ending classifier\n",
    "\n",
    "f = open(\"dialogues_text.txt\", 'r', encoding='utf-8', errors='ignore')\n",
    "\n",
    "convos = f.readlines()\n",
    "ending = []\n",
    "not_ending = []\n",
    "starter = []\n",
    "for convo in convos:\n",
    "    sentences = convo.split('__eou__')[:-1]\n",
    "    ending.append(sentences[-1])\n",
    "    starter.append(sentences[0])\n",
    "    not_ending += sentences[:-1]\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.array(ending+not_ending)\n",
    "y_train_text = np.array([\"e\"]*len(ending)+[\"c\"]*len(not_ending))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train_text, test_size=0.2, random_state=42)\n",
    "\n",
    "target_names = ['e', 'c']\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(y_train)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "classifier.fit(X_train, Y)\n",
    "predicted = classifier.predict(X_test)\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(all_labels)):\n",
    "    if all_labels[i][0] == y_test[i]:\n",
    "        count += 1\n",
    "\n",
    "print(\"accuracy: \", count/len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8591776553719468\n"
     ]
    }
   ],
   "source": [
    "# Prepare question classifier\n",
    "\n",
    "f = open(\"dialogues_text.txt\", 'r', encoding='utf-8', errors='ignore')\n",
    "\n",
    "convos = f.readlines()\n",
    "questions = []\n",
    "non_question = []\n",
    "for convo in convos:\n",
    "    sentences = convo.split('__eou__')[:-1]\n",
    "    for i in range(len(sentences)-1):\n",
    "        if '?' == sentences[i][-2]:\n",
    "            questions.append(sentences[i][:-2])\n",
    "        else:\n",
    "            non_question.append(sentences[i])\n",
    "\n",
    "X_train = np.array(questions+non_question)\n",
    "y_train_text = np.array([\"e\"]*len(questions)+[\"c\"]*len(non_question))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train_text, test_size=0.2, random_state=42)\n",
    "\n",
    "target_names = ['e', 'c']\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(y_train)\n",
    "\n",
    "question_classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "question_classifier.fit(X_train, Y)\n",
    "predicted = question_classifier.predict(X_test)\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(all_labels)):\n",
    "    if all_labels[i][0] == y_test[i]:\n",
    "        count += 1\n",
    "\n",
    "print(\"accuracy: \", count/len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(data_path):\n",
    "    \n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    for line in lines[: min(2000, len(lines) - 1)]:\n",
    "        input_text = line.split('\\t')[0]\n",
    "        target_text = line.split('\\t')[1]\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "   \n",
    "    zippedList =  list(zip(input_texts, target_texts))\n",
    "    lines = pd.DataFrame(zippedList, columns = ['input' , 'output']) \n",
    "    \n",
    "    input_lines = list()\n",
    "    for line in lines.input:\n",
    "        input_lines.append( line ) \n",
    "\n",
    "    tokenizer = preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts( input_lines ) \n",
    "    tokenized_input_lines = tokenizer.texts_to_sequences( input_lines ) \n",
    "\n",
    "    length_list = list()\n",
    "    for token_seq in tokenized_input_lines:\n",
    "        length_list.append( len( token_seq ))\n",
    "    max_input_length = np.array( length_list ).max()\n",
    "    print( 'Input max length is {}'.format( max_input_length ))\n",
    "\n",
    "    padded_input_lines = preprocessing.sequence.pad_sequences( tokenized_input_lines , maxlen=max_input_length , padding='post' )\n",
    "    encoder_input_data = np.array( padded_input_lines )\n",
    "    print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n",
    "\n",
    "    input_word_dict = tokenizer.word_index\n",
    "    num_input_tokens = len( input_word_dict )+1\n",
    "    print( 'Number of Input tokens = {}'.format( num_input_tokens))\n",
    "    \n",
    "    output_lines = list()\n",
    "    for line in lines.output:\n",
    "        output_lines.append( '<START> ' + line + ' <END>' )  \n",
    "\n",
    "    tokenizer = preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts( output_lines ) \n",
    "    tokenized_output_lines = tokenizer.texts_to_sequences( output_lines ) \n",
    "\n",
    "    length_list = list()\n",
    "    for token_seq in tokenized_output_lines:\n",
    "        length_list.append( len( token_seq ))\n",
    "    max_output_length = np.array( length_list ).max()\n",
    "    print( 'Output max length is {}'.format( max_output_length ))\n",
    "\n",
    "    padded_output_lines = preprocessing.sequence.pad_sequences( tokenized_output_lines , maxlen=max_output_length, padding='post' )\n",
    "    decoder_input_data = np.array( padded_output_lines )\n",
    "    print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
    "\n",
    "    output_word_dict = tokenizer.word_index\n",
    "    num_output_tokens = len( output_word_dict )+1\n",
    "    print( 'Number of Output tokens = {}'.format( num_output_tokens))\n",
    "\n",
    "\n",
    "\n",
    "    return input_word_dict, max_input_length, encoder_input_data, decoder_input_data, output_word_dict, max_output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str, input_word_dict, max_input_length ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( input_word_dict[ word ] )\n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chatbot():\n",
    "    enc_model = tf.keras.models.load_model(\"enc_model_q3.h5\")\n",
    "    dec_model = tf.keras.models.load_model(\"dec_model_q3.h5\")\n",
    "    input_word_dict, max_input_length, encoder_input_data, decoder_input_data, output_word_dict, max_output_length = prepare_model(\"q3.txt\")\n",
    "\n",
    "    for epoch in range( encoder_input_data.shape[0] ):\n",
    "        states_values = enc_model.predict( str_to_tokens( input( 'User: ' ), input_word_dict, max_input_length ) )\n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "        empty_target_seq[0, 0] = output_word_dict['start']\n",
    "        stop_condition = False\n",
    "        decoded_translation = ''\n",
    "        while not stop_condition :\n",
    "            dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "            sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "            sampled_word = None\n",
    "            for word , index in output_word_dict.items() :\n",
    "                if sampled_word_index == index :\n",
    "                    decoded_translation += ' {}'.format( word )\n",
    "                    sampled_word = word\n",
    "\n",
    "            if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
    "                stop_condition = True\n",
    "\n",
    "            empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "            empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "            states_values = [ h , c ] \n",
    "\n",
    "        print( \"Bot:\" +decoded_translation.replace(' end', '') )\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(user_in, enc_model, dec_model, input_word_dict, max_input_length, encoder_input_data, decoder_input_data, output_word_dict, max_output_length):\n",
    "\n",
    "    states_values = enc_model.predict( str_to_tokens( user_in, input_word_dict, max_input_length ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = output_word_dict['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in output_word_dict.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "\n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( \"Bot:\" +decoded_translation.replace(' end', '') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_chatbot():\n",
    "    enc_model_q1 = tf.keras.models.load_model(\"enc_model_q1.h5\")\n",
    "    dec_model_q1 = tf.keras.models.load_model(\"dec_model_q1.h5\")\n",
    "    input_word_dict_q1, max_input_length_q1, encoder_input_data_q1, decoder_input_data_q1, output_word_dict_q1, max_output_length_q1 = prepare_model(\"q1.txt\")\n",
    "    \n",
    "    enc_model_q2 = tf.keras.models.load_model(\"enc_model_q2.h5\")\n",
    "    dec_model_q2 = tf.keras.models.load_model(\"dec_model_q2.h5\")\n",
    "    input_word_dict_q2, max_input_length_q2, encoder_input_data_q2, decoder_input_data_q2, output_word_dict_q2, max_output_length_q2 = prepare_model(\"q2.txt\")\n",
    "    \n",
    "    enc_model_q3 = tf.keras.models.load_model(\"enc_model_q3.h5\")\n",
    "    dec_model_q3 = tf.keras.models.load_model(\"dec_model_q3.h5\")\n",
    "    input_word_dict_q3, max_input_length_q3, encoder_input_data_q3, decoder_input_data_q3, output_word_dict_q3, max_output_length_q3 = prepare_model(\"q3.txt\")\n",
    "    \n",
    "    for epoch in range( encoder_input_data_q3.shape[0] ):\n",
    "        user_in = input( 'User: ' )\n",
    "        if question_classifier.predict([user_in])[0][0]==0:\n",
    "            # enter q2: answering question\n",
    "            print(\"currently at q2\")\n",
    "            respond(user_in, enc_model_q2, dec_model_q2, input_word_dict_q2, max_input_length_q2, encoder_input_data_q2, decoder_input_data_q2, output_word_dict_q2, max_output_length_q2)\n",
    "        else:\n",
    "            if classifier.predict([user_in])[0][0]==0:\n",
    "                # enter q1: starting topic\n",
    "                print(\"currently at q1\")\n",
    "                respond(user_in, enc_model_q1, dec_model_q1, input_word_dict_q1, max_input_length_q1, encoder_input_data_q1, decoder_input_data_q1, output_word_dict_q1, max_output_length_q1)\n",
    "            else:\n",
    "                # enter q3: making comment\n",
    "                print(\"currently at q3\")\n",
    "                respond(user_in, enc_model_q3, dec_model_q3, input_word_dict_q3, max_input_length_q3, encoder_input_data_q3, decoder_input_data_q3, output_word_dict_q3, max_output_length_q3)\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose a model to begin your conversation with our chatbot: \n",
      "Press 1 to use the simple encoder-decoder chatbot\n",
      "Press 2 to use the advanced encoder-decoder chatbot which integrated a finite-state automaton\n",
      "Your choice: 2\n",
      "Start to chat with our advanced encoder-decoder chatbot\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Input max length is 256\n",
      "Encoder input data shape -> (2000, 256)\n",
      "Number of Input tokens = 2806\n",
      "Output max length is 61\n",
      "Decoder input data shape -> (2000, 61)\n",
      "Number of Output tokens = 2798\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Input max length is 47\n",
      "Encoder input data shape -> (2000, 47)\n",
      "Number of Input tokens = 1955\n",
      "Output max length is 88\n",
      "Decoder input data shape -> (2000, 88)\n",
      "Number of Output tokens = 2758\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Input max length is 148\n",
      "Encoder input data shape -> (2000, 148)\n",
      "Number of Input tokens = 2935\n",
      "Output max length is 150\n",
      "Decoder input data shape -> (2000, 150)\n",
      "Number of Output tokens = 2561\n",
      "User: hello\n",
      "currently at q3\n",
      "Bot: hello i'd like to have a vacant apartment\n",
      "\n",
      "User: how are you\n",
      "currently at q2\n",
      "Bot: fine and you\n",
      "\n",
      "User: I am good\n",
      "currently at q3\n",
      "Bot: shame on you\n",
      "\n",
      "User: I am fine\n",
      "currently at q3\n",
      "Bot: shame on please\n",
      "\n",
      "User: do you like football\n",
      "currently at q2\n",
      "Bot: very much\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Please choose a model to begin your conversation with our chatbot: ')\n",
    "print('Press 1 to use the simple encoder-decoder chatbot')\n",
    "print('Press 2 to use the advanced encoder-decoder chatbot which integrated a finite-state automaton')\n",
    "user_input = input( \"Your choice: \" )\n",
    "if(user_input == '1'):\n",
    "    print('Start to chat with the simple encoder-decoder chatbot')\n",
    "    simple_chatbot()\n",
    "elif(user_input == '2'):\n",
    "    print('Start to chat with our advanced encoder-decoder chatbot')\n",
    "    advanced_chatbot()\n",
    "else:\n",
    "    print(\"Invalid input, please input only 1 or 2, restart the program to begin your conversation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
